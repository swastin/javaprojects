<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0" uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title type="html">Complex KIE server tests automation part 2: Deadline notifications after KIE server restart</title><link rel="alternate" href="https://blog.kie.org/2021/11/deadline-notifications-after-restart.html" /><author><name>Gonzalo Muñoz Fernández</name></author><id>https://blog.kie.org/2021/11/deadline-notifications-after-restart.html</id><updated>2021-11-16T16:53:42Z</updated><content type="html">“Deadlines keep you on track for success” (Timothy Dalton) MOTIVATION: SHOULDN’T WE AUTOMATE RESILIENCE TESTING AS WELL? This article follows the . In this ocasion, we will cover how to test deadline notifications after KIE server restart. With human task deadlines, you can set a time limit for that task, when not started or completed. Therefore, the KIE server will send a notification (by default, an automated email) to the target people defined for that task as managers, administrators, other team members. Here, we will focus on resilience testing. If the KIE server crashes, as there is a mechanism to persist the timer in a database, the KIE server will recover it after restarting. Consequently, the failover mechanism allows sending the deadline notification (triggered before crashing) at the right timing. The following figure depicts this scenario: This flow involves multiple system testing components: KIE Server, database, and SMTP server for mailing. We might be tempted to do a one-time setup instead of creating an automated test for our CI/CD pipelines. But, in the spirit of Martin Fowler’s soundbite (reduces pain drastically), we will follow the automated approach. This way we will exercise the failover scenario regularly, taking advantage of containers.  First of all, we have to select our tools wisely, as the SMTP server. ONE SMTP SERVER TO TRACK THEM ALL The purpose of notification testing is to ensure that: * right inbox receives the expected emails (no more, no less). * the received emails have the right content for headers and body. So, we need a containerized SMTP server with the following requirements: * Easy to configure: minimum setup for required operations (e.g., authentication is useless to check testing emails) * Fast bootstrap: not too long to be up and running * Clear API to assist test automation: documented REST APIs to retrieve and clear emails. In our case, we have chosen “”, an open-source SMTP server, as it fulfills these expectations and also has a docker image published in . We use the following REST APIs in our automation. Respectively, they retrieve all the messages from the inbox and delete all the messages after each test: Notice that they belong to different versions of the API (there is no DELETE operation at v2). Therefore, two different basePaths (/api/v2, /api/v1) will be set up for each endpoint in the REST-assured RequestSpecification. FITTING CONTAINERS TOGETHER Once we have defined our testing approach with self-unit independent pieces (containers) and the testing frameworks (Junit5 with and ), it is time to write our tests that will be the glue for fitting all the components together.  Our system-under-test -the main component- is the KIE Server. From version 7.61.0.Final, we can download it from the . From a Multistage Dockerfile, a temporary image on-the-fly will be created containing the business application (kjar) for exercising test scenarios. Therefore: * First stage, it will pull the slim maven image and install the kjars tailored for our tests. * Second stage, it will pull the KIE server image (in this case, ) with any additional configuration (jboss-cli scripts for configuring persistence and logging). The mailhog container is a testcontainers GenericContainer (that internally pulls the latest image from docker hub). It is exposing their SMTP and HTTP ports. Notice that it shares the same network as the rest of the containers: communication can occur among them without the need of exposing ports through the host. @Container public static GenericContainer&lt;?&gt; mailhog = new GenericContainer&lt;&gt;("mailhog/mailhog:latest") .withExposedPorts(PORT_SMTP, PORT_HTTP) .withNetwork(network) .withNetworkAliases("mailhog") .withLogConsumer(new Slf4jLogConsumer(logger)) .waitingFor(Wait.forHttp("/")); Finally, the PostgreSQL container is one of the out-of-the-box testcontainers database modules. We will use the initialization script under /docker-entrypoint-initdb.d containing postgresql-jbpm-schema.sql as explained . Maven build-tool pulls the schema from GitHub sources, from the same branch as the system-under-test. It is automatically downloaded using the download-maven-plugin at generate-sources phase, as it is shown in this snippet taken from the pom.xml: &lt;configuration&gt; &lt;url&gt;http://raw.githubusercontent.com/kiegroup/jbpm/${version.org.kie}/jbpm-db-scripts/src/main/resources/db/ddl-scripts/postgresql/postgresql-jbpm-schema.sql&lt;/url&gt; &lt;outputFileName&gt;postgresql-jbpm-schema.sql&lt;/outputFileName&gt; &lt;unpack&gt;false&lt;/unpack&gt; &lt;outputDirectory&gt;${project.build.directory}/postgresql&lt;/outputDirectory&gt; &lt;/configuration&gt; TESTING DEADLINE NOTIFICATIONS AFTER KIE SERVER RESTART This scenario (deadline notifications after KIE server restart) in former releases of KIE Server contained a bug tracked by :  duplicated emails were received in the target inboxes. The root cause was that the initialization of the human task service occurred before the deployment (upon server restart). Therefore, the timer service was not available at the moment of starting deadlines, provoking the problem of double notification. Let’s try to verify that, after fixing, it is working properly. First of all, we need a kjar containing a simple process with a human task with this deadline configuration: We can easily stop the KIE Server container after deploying the kjar and start it again to simulate a crash with a reboot.  And then, after deadlines timeout, we will check the expected outcome by means of REST-assured utils. We can connect to Mailhog to assure that the received emails match the expected ones: given() .spec(specV2) .when() .get("/messages") .then() .body("total", equalTo(3)) .assertThat().body("items[0].Content.Headers.To", hasItem("administrator@jbpm.org")) .assertThat().body("items[0].Content.Headers.From", hasItem("john@jbpm.org")) .assertThat().body("items[0].Content.Headers.Subject", hasItem("foo")) .assertThat().body("items[0].Content.Body", is("bar")); We also have another test for checking the case when a different kjar is deployed after the KIE server restart, then, no notification is sent because the kjar that triggered the deadline is not deployed again. Finally, you can find the code and configuration for this example . CONCLUSION: AUTOMATED TESTS FOR DEADLINE NOTIFICATIONS AFTER KIE SERVER RESTART Automated Tests are also a good option to make resilience and integration testing less painful. We can combine different dockerized components (from the system-under-test -KIE server- to external databases -PostgreSQL- or SMTP servers -Mailhog-) into JUnit tests. As we control the containers’ lifecycle, it is easy to stop/start them to check failover mechanisms. The post appeared first on .</content><dc:creator>Gonzalo Muñoz Fernández</dc:creator></entry><entry><title>Custom JFR event templates with Cryostat 2.0</title><link rel="alternate" href="https://developers.redhat.com/articles/2021/11/16/custom-jfr-event-templates-cryostat-20" /><author><name>Andrew Azores, Elliott Baron</name></author><id>e48bf3e8-2a3c-4f9d-82e8-ad6ba4e344a5</id><updated>2021-11-16T07:00:00Z</updated><published>2021-11-16T07:00:00Z</published><summary type="html">&lt;p&gt;Welcome back to our series of hands-on introductions to using &lt;a href="https://developers.redhat.com/articles/2021/10/18/announcing-cryostat-20-jdk-flight-recorder-containers"&gt;Cryostat 2.0&lt;/a&gt;. This article shows you how to preconfigure custom &lt;a href="https://github.com/cryostatio/cryostat#event-templates"&gt;event templates&lt;/a&gt; for Java application monitoring with JDK Flight Recorder (JFR). First, you'll use the new &lt;a href="https://catalog.redhat.com/software/operators/detail/60ee049a744684587e218ef5"&gt;Cryostat Operator&lt;/a&gt; and a Red Hat OpenShift &lt;code&gt;ConfigMap&lt;/code&gt; to create a custom event template, then you'll instruct the Cryostat Operator to use the &lt;code&gt;ConfigMap&lt;/code&gt; when deploying Cryostat. You can use the OpenShift console to interact with Cryostat or edit the YAML file for your Cryostat custom resource. We'll demonstrate both approaches.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: Cryostat is JDK Flight Recorder for &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; or &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt;. The &lt;a href="https://access.redhat.com/documentation/en-us/openjdk/11/html/release_notes_for_cryostat_2.0"&gt;Red Hat build of Cryostat 2.0&lt;/a&gt; is now widely available in technology preview. Cryostat 2.0 introduces many new features and improvements, such as automated rules, a better API response JSON format, custom targets, concurrent target JMX connections, WebSocket push notifications, and more. The Red Hat build includes the &lt;a href="https://catalog.redhat.com/software/operators/detail/60ee049a744684587e218ef5"&gt;Cryostat Operator&lt;/a&gt; to simplify and automate Cryostat deployment on OpenShift.&lt;/p&gt; &lt;h2&gt;Create a custom template and ConfigMap&lt;/h2&gt; &lt;p&gt;With Cryostat, you can &lt;a href="https://cryostat.io/guides/#download-edit-and-upload-a-customized-event-template"&gt;download&lt;/a&gt; existing template files directly from a Java virtual machine (JVM). Once you've downloaded a template file to your local machine, you can use the &lt;a href="https://cryostat.io/guides/#edit-template-with-jmc"&gt;JDK Mission Control&lt;/a&gt; (JMC) Template Manager to easily edit the template to suit your needs.&lt;/p&gt; &lt;p&gt;After customizing the template file, it's fairly simple to create a &lt;code&gt;ConfigMap&lt;/code&gt; from it. This &lt;code&gt;ConfigMap&lt;/code&gt; stores the template file inside of the cluster where Cryostat will run. You can use &lt;code&gt;oc&lt;/code&gt; or &lt;code&gt;kubectl&lt;/code&gt; in the namespace where Cryostat is to be deployed. Here's the source if you use the former:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt; $ oc create configmap my-template --from-file=/path/to/custom.jfc &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This command creates a &lt;code&gt;ConfigMap&lt;/code&gt; named &lt;code&gt;my-template&lt;/code&gt; from a file on your local machine, located at &lt;code&gt;/path/to/custom.jfc&lt;/code&gt;. The file will be placed inside the &lt;code&gt;ConfigMap&lt;/code&gt; under the &lt;code&gt;custom.jfc&lt;/code&gt; filename.&lt;/p&gt; &lt;h2&gt;Add your ConfigMap to the Cryostat Operator&lt;/h2&gt; &lt;p&gt;Once you've created a &lt;code&gt;ConfigMap&lt;/code&gt;, you only need to instruct the Cryostat Operator to use it when deploying Cryostat. You can do this when you create a Cryostat custom resource, or by updating an existing one. If you're installing Cryostat using the OpenShift console, select &lt;strong&gt;Event Templates&lt;/strong&gt; under the Cryostat custom resource that you want to update. Choose &lt;strong&gt;Add Event Template&lt;/strong&gt;. The &lt;strong&gt;Config Map Name&lt;/strong&gt; drop-down list will display all &lt;code&gt;ConfigMap&lt;/code&gt;s in the local namespace. Select the &lt;code&gt;ConfigMap&lt;/code&gt; containing the event template you just created. For &lt;strong&gt;Filename&lt;/strong&gt;, enter the name of the &lt;code&gt;.jfc&lt;/code&gt; file within the &lt;code&gt;ConfigMap&lt;/code&gt;. In Figure 1, we're using &lt;code&gt;custom.jfc&lt;/code&gt;, which we created in the previous section.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/create-cryostat-template-config-map.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/create-cryostat-template-config-map.png?itok=McevdC0z" width="600" height="236" alt="Creating an OpenShift ConfigMap containing a JDK .jfc event template definition file." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1. Create a ConfigMap with a JDK .jfc event template definition file. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;You can also work directly with the YAML for the Cryostat custom resource. The process is similar to what you did in the console. Add an &lt;code&gt;eventTemplates&lt;/code&gt; property to the &lt;code&gt;spec&lt;/code&gt; section, if one isn't already present. Then, add an array entry with &lt;code&gt;configMapName&lt;/code&gt; referencing the name of the &lt;code&gt;ConfigMap&lt;/code&gt;, and a &lt;code&gt;filename&lt;/code&gt; referencing the filename within the &lt;code&gt;ConfigMap&lt;/code&gt;. The YAML below mirrors what we did in the OpenShift console example in Figure 1.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt; apiVersion: operator.cryostat.io/v1beta1 kind: Cryostat metadata:   name: cryostat-sample spec:   eventTemplates:    - configMapName: custom-template     filename: my-template.jfc &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Once you've saved your changes to the Cryostat custom resource, the Cryostat Operator will deploy Cryostat with this template preconfigured. Visiting the Cryostat web application will show that the template is present and available to use for creating new JDK flight recordings, as you can see in Figure 2.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/cryostat-template-from-config-map.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/cryostat-template-from-config-map.png?itok=FxLG5uud" width="600" height="136" alt="The Event Template list displays the template created via ConfigMap." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2. The Event Template list displays the template created via ConfigMap. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;In this article, you've learned how to preconfigure Cryostat with customized event templates using OpenShift &lt;code&gt;ConfigMap&lt;/code&gt;s. You can use customized templates to gather the specific data you need in your JDK flight recordings. Visit &lt;a href="http://cryostat.io/"&gt;Cryostat.io&lt;/a&gt; and see the other articles in this series for further details:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://developers.redhat.com/blog/2021/01/25/introduction-to-containerjfr-jdk-flight-recorder-for-containers"&gt;Introduction to Cryostat: JDK Flight Recorder for containers&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/10/18/announcing-cryostat-20-jdk-flight-recorder-containers"&gt;Get started with Cryostat 2.0&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/10/26/configuring-java-applications-use-cryostat"&gt;Configuring Java applications to use Cryostat&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/02/java-monitoring-custom-targets-cryostat"&gt;Java monitoring for custom targets with Cryostat&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="ADD_URL"&gt;Automating JDK Flight Recorder in containers&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/11/16/custom-jfr-event-templates-cryostat-20" title="Custom JFR event templates with Cryostat 2.0"&gt;Custom JFR event templates with Cryostat 2.0&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Andrew Azores, Elliott Baron</dc:creator><dc:date>2021-11-16T07:00:00Z</dc:date></entry><entry><title>.NET 6 now available for RHEL and OpenShift</title><link rel="alternate" href="https://developers.redhat.com/articles/2021/11/15/net-60-now-available-rhel-and-openshift" /><author><name>Mauricio "Maltron" Leal</name></author><id>871d7b48-0893-4b7c-8e36-132292329b44</id><updated>2021-11-15T20:00:00Z</updated><published>2021-11-15T20:00:00Z</published><summary type="html">&lt;p&gt;.NET 6 is now generally available on &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux (RHEL)&lt;/a&gt; 7, RHEL 8, and &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;. Here's a quick overview of what developers need to know about this new major release.&lt;/p&gt; &lt;h2&gt;New features in .NET 6&lt;/h2&gt; &lt;p&gt;In addition to x64 architecture (64-bit Intel/AMD), &lt;a href="https://developers.redhat.com/topics/dotnet/"&gt;.NET&lt;/a&gt; is now also available for ARM64 (64-bit ARM), and s390x (64-bit IBM Z) architectures.&lt;/p&gt; &lt;p&gt;.NET 6 includes new language versions &lt;a href="https://docs.microsoft.com/en-us/dotnet/csharp/whats-new/csharp-10)"&gt;C# 10&lt;/a&gt; and &lt;a href="https://devblogs.microsoft.com/dotnet/whats-new-in-fsharp-6/"&gt;F# 6&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;ASP.NET Core adds a new &lt;a href="https://devblogs.microsoft.com/aspnet/asp-net-core-updates-in-net-6-preview-4/#introducing-minimal-apis"&gt;minimal API&lt;/a&gt; that leverages new C# 10 features to write web applications with less code.&lt;/p&gt; &lt;p&gt;Like previous versions, .NET 6 brings many &lt;a href="https://devblogs.microsoft.com/dotnet/performance-improvements-in-net-6/"&gt;performance improvements&lt;/a&gt; to the base libraries, GC and JIT.&lt;/p&gt; &lt;p&gt;.NET 6 introduces source generators for &lt;a href="https://devblogs.microsoft.com/dotnet/announcing-net-6-preview-4/#microsoft-extensions-logging-compile-time-source-generator"&gt;logging&lt;/a&gt; and &lt;a href="https://devblogs.microsoft.com/dotnet/try-the-new-system-text-json-source-generator/"&gt;JSON&lt;/a&gt;. Thanks to these generators, JSON serialization and logging can be performed with less allocation and better performance.&lt;/p&gt; &lt;h2&gt;How to install .NET 6&lt;/h2&gt; &lt;p&gt;You can install .NET 6 on RHEL 7 (x64 only) with the usual command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-html"&gt;# yum install rh-dotnet60&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;On RHEL 8 (for x64, arm64, and s390x), enter:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-html"&gt;# dnf install dotnet-sdk-6.0&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The .NET 6 SDK and runtime container images are available from the Red Hat Container Registry. You can use the container images as standalone images and with OpenShift on all supported architectures:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ podman run --rm registry.redhat.io/ubi8/dotnet-60 dotnet --version 6.0.100&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Long-term support for .NET 6&lt;/h2&gt; &lt;p&gt;.NET 6 is a long-term support (LTS) release. It will be supported for three years, until November 2024.&lt;/p&gt; &lt;p&gt;Based on the .NET release schedule, the next version of .NET, .NET 7, is not an LTS release. It will be released in November 2022 and supported for 18 months until May 2024.&lt;/p&gt; &lt;p&gt;The next LTS release is .NET 8, which will be released in November 2023.&lt;/p&gt; &lt;p&gt;The existing .NET Core 3.1 and .NET 5 releases will be supported until December 2022 and May 2022, respectively.&lt;/p&gt; &lt;h2&gt;Where to learn more&lt;/h2&gt; &lt;p&gt;Visit the &lt;a href="http://redhatloves.net/"&gt;.NET overview&lt;/a&gt; page to find out more about using .NET on Red Hat Enterprise Linux and OpenShift. You can also explore &lt;a href="https://developers.redhat.com/topics/dotnet"&gt;more .NET resources on Red Hat Developer&lt;/a&gt;:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Learn &lt;a href="https://developers.redhat.com/blog/2021/03/16/three-ways-to-containerize-net-applications-on-red-hat-openshift#"&gt;three ways to containerize .NET applications on Red Hat OpenShift&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/07/07/deploy-net-applications-red-hat-openshift-using-helm"&gt;Deploy .NET applications on Red Hat OpenShift using Helm&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/11/15/net-60-now-available-rhel-and-openshift" title=".NET 6 now available for RHEL and OpenShift"&gt;.NET 6 now available for RHEL and OpenShift&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Mauricio "Maltron" Leal</dc:creator><dc:date>2021-11-15T20:00:00Z</dc:date></entry><entry><title type="html">Monitoring Quarkus runtime with DevUI</title><link rel="alternate" href="http://www.mastertheboss.com/soa-cloud/quarkus/monitoring-quarkus-runtime-with-devui/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=monitoring-quarkus-runtime-with-devui" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/soa-cloud/quarkus/monitoring-quarkus-runtime-with-devui/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=monitoring-quarkus-runtime-with-devui</id><updated>2021-11-15T17:26:22Z</updated><content type="html">Quarkus now includes (since version 2.3.0) a Development UI (DevUI) which allows us to monitor runtime information about your extensions. Quarkus DevUI is an experimental feature you can use to collect inner details of your Quarkus environment. You can use for different purposes such as: Application introspection, to see the list of Beans / Observers ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>Red Hat Software Collections 3.8 and Red Hat Developer Toolset 11 now generally available</title><link rel="alternate" href="https://developers.redhat.com/articles/2021/11/15/red-hat-software-collections-38-and-red-hat-developer-toolset-11-now-generally" /><author><name>Brian Gollaher</name></author><id>62c08ed0-9672-4d63-92c8-4441f1f231cb</id><updated>2021-11-15T07:00:00Z</updated><published>2021-11-15T07:00:00Z</published><summary type="html">&lt;p&gt;The latest versions of&lt;a href="https://developers.redhat.com/products/softwarecollections/overview"&gt; Red Hat Software Collections&lt;/a&gt; and&lt;a href="https://developers.redhat.com/products/developertoolset/overview"&gt; Red Hat Developer Toolset&lt;/a&gt; for &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; (RHEL) 7 are now generally available. Software Collections 3.8 delivers the latest stable versions of many popular open source runtime languages, web servers, and databases natively to the &lt;a href="https://www.redhat.com/en/technologies/linux-platforms/enterprise-linux"&gt;world’s leading enterprise Linux platform&lt;/a&gt;. These components are supported for up to five years, helping to enable a more consistent, efficient, and reliable developer experience.&lt;/p&gt; &lt;p&gt;Here's a quick overview of the updated development tools and collections you'll find in this release.&lt;/p&gt; &lt;h2 id="updates_to_red_hat_software_collections-h2"&gt;Updates to Red Hat Software Collections&lt;/h2&gt; &lt;p&gt;New and updated collections in the latest release of Red Hat Software Collections include:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;&lt;strong&gt;Nginx 1.20:&lt;/strong&gt; This is a new release of Nginx, a web and proxy server with a focus on high concurrency, performance, and low memory usage. This version supports client SSL certificate validation with Online Certificate Status Protocol (OCSP) and improves support for HTTP/2.&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;strong&gt;Redis 6:&lt;/strong&gt; This release of Redis, a persistent key-value database, supports SSL on all channels, as well as access control list and Redis Serialization Protocol version 3.&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;strong&gt;JDK Mission Control 8.0.1 (update):&lt;/strong&gt; JDK Mission Control is an advanced set of tools for managing, monitoring, profiling, and troubleshooting &lt;a href="https://developers.redhat.com/topics/enterprise-java/"&gt;Java&lt;/a&gt; applications. This update to JMC 8 delivers a number of important bug and security fixes.&lt;/li&gt; &lt;/ul&gt;&lt;h2 id="updates_to_red_hat_developer_toolset-h2"&gt;Updates to Red Hat Developer Toolset&lt;/h2&gt; &lt;p&gt;Also new in Red Hat Software Collections 3.8 is Developer Toolset 11, which brings an updated, curated collection of &lt;a href="https://developers.redhat.com/products/gcc-clang-llvm-go-rust/overview"&gt;compilers&lt;/a&gt;, toolchains, debuggers, and other critical development tools. Forming the foundation of Developer Toolset 11 is GCC 11.2, a new update of the popular open source compiler collection. Additional updates in Developer Toolset 11 deliver new features to &lt;a href="https://developers.redhat.com/topics/c/"&gt;C/C++&lt;/a&gt; and Fortran debugging and performance tools.&lt;/p&gt; &lt;p&gt;In addition to the Developer Toolset, other compiler toolsets available in RHEL developer tools are updated with this release. Go Toolset is updated to version 1.16, LLVM Toolset is updated to version 12.0, and Rust Toolset is updated to version 1.54.&lt;/p&gt; &lt;p&gt;New collections in Red Hat Software Collections 3.8 are also available as&lt;a href="https://connect.redhat.com/explore/red-hat-container-certification"&gt; Red Hat Certified Containers&lt;/a&gt; through the &lt;a href="https://catalog.redhat.com/software/containers/explore"&gt;Red Hat Ecosystem Catalog&lt;/a&gt;. This makes it easier to build and deploy mission-critical applications using the supported components of Red Hat Software Collections for Red Hat Enterprise Linux and&lt;a href="https://developers.redhat.com/topics/kubernetes/"&gt; Red Hat OpenShift&lt;/a&gt; environments.&lt;/p&gt; &lt;h2&gt;How to access Red Hat Software Collections 3.8&lt;/h2&gt; &lt;p&gt;Red Hat Software Collections 3.8 continues Red Hat’s commitment to customer choice in terms of the underlying compute architecture, with availability across x86_64, ppc64, ppc64le, and s390x hardware.&lt;/p&gt; &lt;p&gt;Red Hat customers with active&lt;a href="https://developers.redhat.com/blog/2019/08/21/why-you-should-be-developing-on-red-hat-enterprise-linux/"&gt; Red Hat Enterprise Linux&lt;/a&gt; subscriptions can access Red Hat Software Collections via the&lt;a href="https://access.redhat.com/solutions/472793"&gt; Red Hat Software Collections repository&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;For more information, please read the full&lt;a href="https://access.redhat.com/documentation/en-us/red_hat_software_collections/3-beta/"&gt; &lt;/a&gt;&lt;a href="https://access.redhat.com/documentation/en-us/red_hat_software_collections/3/"&gt;release notes&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/11/15/red-hat-software-collections-38-and-red-hat-developer-toolset-11-now-generally" title="Red Hat Software Collections 3.8 and Red Hat Developer Toolset 11 now generally available"&gt;Red Hat Software Collections 3.8 and Red Hat Developer Toolset 11 now generally available&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Brian Gollaher</dc:creator><dc:date>2021-11-15T07:00:00Z</dc:date></entry><entry><title type="html">Quarkus 2.4.2.Final released - Maintenance release</title><link rel="alternate" href="https://quarkus.io/blog/quarkus-2-4-2-final-released/" /><author><name>Guillaume Smet</name></author><id>https://quarkus.io/blog/quarkus-2-4-2-final-released/</id><updated>2021-11-12T00:00:00Z</updated><content type="html">Today, we released Quarkus 2.4.2.Final, a maintenance release for our 2.4 release train containing bugfixes and documentation improvements. It is a safe upgrade for anyone already using 2.4. If you are not using 2.4 already, please refer to the 2.4 migration guide. Full changelog You can get the full changelog...</content><dc:creator>Guillaume Smet</dc:creator></entry><entry><title type="html">Getting started with Hibernate reactive on Quarkus</title><link rel="alternate" href="http://www.mastertheboss.com/soa-cloud/quarkus/getting-started-with-hibernate-reactive/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=getting-started-with-hibernate-reactive" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/soa-cloud/quarkus/getting-started-with-hibernate-reactive/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=getting-started-with-hibernate-reactive</id><updated>2021-11-11T19:30:19Z</updated><content type="html">This tutorial will introduce you to Hibernate Reactive which enables support for non-blocking database drivers and a reactive programming with Hibernate ORM. Uni and Multi streams Persistence operations are designed to use blocking IO for interaction with the database, and are therefore not appropriate for use in a reactive environment. Hibernate Reactive is the first ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title type="html">Red Hat Summit Connect and Partner Experience Dublin - Talking Architecture Shop (slides)</title><link rel="alternate" href="http://www.schabell.org/2021/11/red-hat-summit-connect-and-partner-experience-slides.html" /><author><name>Eric D. Schabell</name></author><id>http://www.schabell.org/2021/11/red-hat-summit-connect-and-partner-experience-slides.html</id><updated>2021-11-11T17:30:00Z</updated><content type="html">The  and events for customers were held this week in Dublin. Each was a one day event,  part of a new series of small-scale events, that brings the discussion of open source technology to your local cities. The days were intended to bring updates and insights into the latest technologies and also offered the opportunity to get hands on with a few Red Hat technologies.  There were different streams of session topics hosting tech talks led by Red Hat experts and also business focused sessions delivered from local industry leaders including some fantastic partner and customer stories. I was invited to give a session at both events in Dublin this week and wanted to share the slides presented that included links to all of the available content we discussed.  As a refresh on the day, we shared our insights into our larger architectures exploring how open source can be used successfully at scale. Below you will find the slides: Provided here as a reminder, the title and abstract: You've heard of large scale open source architectures, but have you ever wanted to take a serious look at these real life enterprise implementations that scale? This session takes attendees on a tour of multiple use cases covering enterprise challenges like integration, optimisation, cloud adoption, hybrid cloud management, and much more. Not only are these architectures interesting, but they are successful real life implementations featuring open source technologies and power many of your own online experiences.  The attendee departs this session with a working knowledge of how to map general open source technologies to their solutions. Material covered is available freely online and attendees can use these solutions as starting points for aligning to their own solution architectures. Join us for an hour of power as we talk architecture shop!  Hope you enjoyed the session and if you were not able to attend, maybe we can meet and chat about this in person someday in the near future.</content><dc:creator>Eric D. Schabell</dc:creator></entry><entry><title>Best practices for building images that pass Red Hat Container Certification</title><link rel="alternate" href="https://developers.redhat.com/articles/2021/11/11/best-practices-building-images-pass-red-hat-container-certification" /><author><name>clobner</name></author><id>a0944e58-5f02-4463-bf78-07a729bf03bb</id><updated>2021-11-11T10:00:00Z</updated><published>2021-11-11T10:00:00Z</published><summary type="html">&lt;p&gt;Building unique images for various &lt;a href="https://developers.redhat.com/topics/containers"&gt;container&lt;/a&gt; orchestrators can be a maintenance and testing headache. A better idea is to build a single image that takes full advantage of the vendor support and security built into &lt;a href="https://developers.redhat.com/products/openshift"&gt;Red Hat OpenShift&lt;/a&gt;, and that also runs well in &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;A universal application image (UAI) is an image that uses &lt;a href="https://developers.redhat.com/products/rhel/ubi"&gt;Red Hat Universal Base Image (UBI)&lt;/a&gt; from &lt;a href="https://developers.redhat.com/products/rhel"&gt;Red Hat Enterprise Linux&lt;/a&gt; as its foundation. The UAI also includes the application being deployed, adds extra elements that make it more secure and scalable in Kubernetes and OpenShift, and can pass &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/13/html/partner_integration/building-certified-container-images"&gt;Red Hat Container Certification&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;This article introduces you to nine best practices you should incorporate into your Dockerfile when building a UAI. Each section in this article explains a practice, shows you how to implement the practice, and includes Red Hat certification requirements related to the topic.&lt;/p&gt; &lt;h2&gt;Best practice #1: Choose a Universal Base Image&lt;/h2&gt; &lt;p&gt;The base image for your application provides the &lt;a href="https://developers.redhat.com/topics/linux"&gt;Linux&lt;/a&gt; libraries required by the application. The base image that you choose affects the versatility, security, and efficiency of your container.&lt;/p&gt; &lt;p&gt;A Red Hat UBI enables your universal application image to run well in both Kubernetes and OpenShift, is compliant with the &lt;a href="https://opencontainers.org/"&gt;Open Container Initiative (OCI)&lt;/a&gt;, is freely redistributable, and receives official Red Hat support when run in OpenShift.&lt;/p&gt; &lt;h3&gt;Building from a UBI&lt;/h3&gt; &lt;p&gt;In your Dockerfile, the &lt;code&gt;FROM&lt;/code&gt; command should create your image from a ubi8 base image. If your build machine has an internet connection, it can download the base image directly from Red Hat's registry like this:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;FROM registry.access.redhat.com/ubi8/openjdk-11:1.3-15&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Red Hat Container Certification requirement&lt;/h3&gt; &lt;p&gt;The base image's Linux libraries must come from Red Hat Enterprise Linux. Red Hat Enterprise Linux's base images and Universal Base Images meet this requirement. See &lt;a href="https://docs.openshift.com/container-platform/4.6/architecture/understanding-development.html#base-image-options"&gt;base image options&lt;/a&gt; in the Red Hat documentation for more information.&lt;/p&gt; &lt;p&gt;Red Hat's base images are available from the &lt;a href="https://catalog.redhat.com/software/containers/explore"&gt;Red Hat certified container images registry&lt;/a&gt;. The images in the ubi8 namespace incorporate libraries from a newer version of Red Hat Enterprise Linux than the ubi7 images. There are UBIs for several different language runtimes.&lt;/p&gt; &lt;p&gt;If Red Hat doesn't offer a UBI for the language runtime you need, start from the smallest ubi8 base image (&lt;code&gt;registry.access.redhat.com/ubi8/ubi-minimal&lt;/code&gt;) and run the commands to install the language's runtime.&lt;/p&gt; &lt;p&gt;The &lt;a href="https://redhat-connect.gitbook.io/partner-guide-for-red-hat-openshift-and-container/program-on-boarding/containers-with-red-hat-universal-base-image-ubi"&gt;Containers and the Red Hat Universal Base Images (UBI)&lt;/a&gt; partner guide offers more information about UBI and related topics. You can also check out the &lt;a href="https://developers.redhat.com/books/red-hat-universal-base-images-ubi"&gt;Red Hat Universal Base Images (UBI)&lt;/a&gt; ebook.&lt;/p&gt; &lt;h2&gt;Best practice #2: Make the image run as a non-root user&lt;/h2&gt; &lt;p&gt;If a process running as root breaks out of the container, it gains root (privileged) access to the host machine. Therefore, run the container as a non-root user so that, if the process breaks out of the container, its access to the host machine is much more limited.&lt;/p&gt; &lt;p&gt;By default, Docker builds and runs an image as root (that is, &lt;code&gt;UID=0&lt;/code&gt;). To avoid this, the Dockerfile for building the image should specify a user ID other than &lt;code&gt;0&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;When Kubernetes runs the container, its processes run as the user ID specified in the Dockerfile.&lt;/p&gt; &lt;h3&gt;Running the image as a non-root user&lt;/h3&gt; &lt;p&gt;To specify a user in a Dockerfile, add the &lt;code&gt;USER&lt;/code&gt; command, such as &lt;code&gt;USER 1001&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;The &lt;a href="https://docs.docker.com/develop/develop-images/dockerfile_best-practices/#user"&gt;Dockerfile best practices&lt;/a&gt; page points out that if you specify the user as its UID instead of a username, you don't need to add the user or group name to the system's &lt;code&gt;passwd&lt;/code&gt; or &lt;code&gt;group&lt;/code&gt; file. However, if the base image sets a good non-root user name, you should specify that user's name. For example, a UBI defines a user named &lt;code&gt;default&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;Red Hat Container Certification requirement&lt;/h3&gt; &lt;p&gt;Red Hat recommends that the image specify a non-root user. When its container is run in OpenShift, the container orchestrator will definitely run its processes as an &lt;a href="https://docs.openshift.com/container-platform/4.6/openshift_images/create-images.html#images-create-guide-openshift_create-images"&gt;arbitrary non-root user&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;When you build an image on a Red Hat UBI that includes a language runtime, the user is already switched to a non-root user named &lt;code&gt;default&lt;/code&gt;.&lt;/p&gt; &lt;h2&gt;Best practice #3: Set group ownership and file permissions&lt;/h2&gt; &lt;p&gt;If a process needs access to files in the container's local file system, the process's user and group should own those files so they are accessible. For OpenShift, the user that runs a container is assigned arbitrarily, but that arbitrary user is always a member of the root group, so you should assign the root group ownership of the local files so that the arbitrary user has access.&lt;/p&gt; &lt;h3&gt;Setting group ownership and file permissions&lt;/h3&gt; &lt;p&gt;In the Dockerfile, set permissions on the directories and files that the process uses. The root group must own those files and be able to read and write them as needed. The Dockerfile code looks like the following, where &lt;code&gt;/some/directory&lt;/code&gt; is the directory with the files that the process needs access to:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;RUN chown -R 0 /some/directory &amp;&amp; \ chmod -R g=u /some/directory&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For compatibility with Kubernetes, the Dockerfile should specify a non-root user ID, then set file ownership to that user ID and the root group:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;USER 1001 RUN chown -R 1001:0 /some/directory&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;These two approaches combined work for both Kubernetes and OpenShift:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;USER 1001 RUN chown -R 1001:0 /some/directory &amp;&amp; \ chmod -R g=u /some/directory&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For example, if the Cassandra database is configured to store its data in the &lt;code&gt;/etc/cassandra&lt;/code&gt; directory, the Dockerfile to build the image for OpenShift needs the following statements:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;USER 1001 RUN chown -R 1001:0 /etc/cassandra &amp;&amp; \ chmod -R g=u /etc/cassandra&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Red Hat Container Certification requirement&lt;/h3&gt; &lt;p&gt;Red Hat Container Certification does not require or exclude setting group ownership and file permissions.&lt;/p&gt; &lt;p&gt;Pods run in an OpenShift cluster as &lt;a href="https://developer.ibm.com/learningpaths/universal-application-image/design-universal-image/#2-design-the-image-to-run-as-a-non-root-user-id"&gt;arbitrary user IDs&lt;/a&gt; that are members of the root group. &lt;a href="https://docs.openshift.com/container-platform/4.6/openshift_images/create-images.html"&gt;OpenShift Container Platform specific guidelines&lt;/a&gt; specify the following:&lt;/p&gt; &lt;p class="Indent1"&gt;For an image to support running as an arbitrary user, directories and files that are written to by processes in the image must be owned by the root group and be read/writable by that group. Files to be executed must also have group execute permissions.&lt;/p&gt; &lt;p&gt;If a process shares files with other processes and therefore needs to run as the specific user or group that owns those files, its pod must define a security context that species the user and group. Also, the cluster must define a set of security context constraints that allow that user and group to be specified. For details, see &lt;a href="https://github.ibm.com/TT-ISV-org/scc/blob/main/README.md"&gt;Getting started with security context constraints on Red Hat OpenShift&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Best practice #4: Build images in multiple stages&lt;/h2&gt; &lt;p&gt;While the deployment image must contain the application and its language runtime, it should not add any tools that are used to build the application or any libraries that are not needed by the running application. Instead, create a two-stage Dockerfile that uses separate images: one image to build artifacts and another to host the application.&lt;/p&gt; &lt;h3&gt;Building an image in multiple stages&lt;/h3&gt; &lt;p&gt;To build an image in multiple stages, the Dockerfile specifies multiple &lt;code&gt;FROM&lt;/code&gt; lines, one at the beginning of each stage. The last stage produces the resulting image file, and the build process discards the images from the earlier stages. Typically, every stage but the last is given a name that makes it easier for a later stage to refer to an earlier stage's artifacts.&lt;/p&gt; &lt;p&gt;For example, a multi-stage build typically consists of two stages, one that builds application artifacts and another that builds the application image. The first stage is typically named &lt;code&gt;builder&lt;/code&gt;. The Docker &lt;a href="https://docs.docker.com/develop/develop-images/multistage-build/"&gt;Use multi-stage builds&lt;/a&gt; pages shows this example:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;FROM golang:1.7.3 AS builder WORKDIR /go/src/github.com/alexellis/href-counter/ RUN go get -d -v golang.org/x/net/html COPY app.go . RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o app . FROM alpine:latest RUN apk --no-cache add ca-certificates WORKDIR /root/ COPY --from=builder /go/src/github.com/alexellis/href-counter/app . CMD ["./app"]&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The stages work as follows:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;The first stage is named &lt;code&gt;builder&lt;/code&gt; and starts from a Go image that already has the Go build tools installed.&lt;/li&gt; &lt;li&gt;The second stage, which builds the deployment image, starts from an Alpine image and does not need a name.&lt;/li&gt; &lt;li&gt;The first stage builds the app named &lt;code&gt;app&lt;/code&gt; in the directory named &lt;code&gt;/go/src/github.com/alexellis/href-counter/&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;The second stage copies the file &lt;code&gt;/go/src/github.com/alexellis/href-counter/app&lt;/code&gt; from the &lt;code&gt;builder&lt;/code&gt; stage into the deployment stage's current directory.&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Red Hat Container Certification requirement&lt;/h3&gt; &lt;p&gt;Red Hat Container Certification does not require or exclude the use of a multi-stage Dockerfile.&lt;/p&gt; &lt;h2&gt;Best practice #5: Include the latest security updates in your image&lt;/h2&gt; &lt;p&gt;The Linux libraries in an image should contain the latest security patches that are available when the image is built. To get the patches:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;Use the latest release of a base image. &lt;/strong&gt;This release should contain the latest security patches available when the base image is built. When a new release of the base image is available, rebuild the application image to incorporate the base image's latest release, because that release contains the latest fixes.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Conduct vulnerability scanning.&lt;/strong&gt; Scan a base or application image to confirm that it doesn't contain any known security vulnerabilities. Commonly used scanning tools include &lt;a href="https://aquasecurity.github.io/trivy/v0.17.0/"&gt;Trivy&lt;/a&gt;, &lt;a href="https://quay.github.io/clair/"&gt;Clair&lt;/a&gt;, and &lt;a href="https://cloud.ibm.com/docs/Registry?topic=va-va_index"&gt;Vulnerability Advisor&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Apply patches.&lt;/strong&gt; Update the Linux components in an image using the operating system's package manager. The package managers for Red Hat Linux are &lt;code&gt;yum&lt;/code&gt; and &lt;code&gt;dnf&lt;/code&gt;. The Dockerfile can run the package manager as part of building the image.&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Building an image with the latest security updates&lt;/h3&gt; &lt;p&gt;Build your application image from the latest release of a UBI, which should include components with the latest security patches.&lt;/p&gt; &lt;p&gt;If a UBI needs newer components because they contain newer security patches, use the &lt;code&gt;RUN&lt;/code&gt; command to update the UBI with the latest security updates like this:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;FROM registry.access.redhat.com/ubi8/openjdk-11:1.3-15 USER root RUN dnf -y update-minimal --security --sec-severity=Important --sec-severity=Critical &amp;&amp; \ dnf clean all USER default&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The UBI already contains the latest security patches that are available at the time the image was built, but this installs any updates that are newer than the image.&lt;/p&gt; &lt;h3&gt;Red Hat Container Certification requirement&lt;/h3&gt; &lt;p&gt;To pass Red Hat Container Certification, Red Hat components in the container image cannot contain any critical or important security vulnerabilities at the time that it is certified.&lt;/p&gt; &lt;p&gt;&lt;a href="https://access.redhat.com/security/updates/classification"&gt;Understanding Red Hat security ratings&lt;/a&gt; explains these different security levels. To update the Red Hat components with security fixes that are not already installed, use this command in the Dockerfile for your image:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;RUN yum -y update-minimal --security --sec-severity=Important --sec-severity=Critical&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Best practice #6: Embed identifying information inside your image&lt;/h2&gt; &lt;p&gt;You should build images that are clearly identifiable, to make it easy for the user to determine the name of the image, who built it, and what it does. This information should be an immutable part of the image that cannot be separated.&lt;/p&gt; &lt;h3&gt;Labeling your image&lt;/h3&gt; &lt;p&gt;Labels are set in the Dockerfile using the &lt;code&gt;LABEL&lt;/code&gt; command. For example, here's how to set the labels required for Red Hat image certification:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;LABEL name="my-namespace/my-image-name" \ vendor="My Company, Inc." \ version="1.2.3" \ release="45" \ summary="Web search application" \ description="This application searches the web for interesting stuff."&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Red Hat Container Certification requirement&lt;/h3&gt; &lt;p&gt;Red Hat Container Certification requires the following labels in your image:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;name&lt;/code&gt;: The name of the image.&lt;/li&gt; &lt;li&gt;&lt;code&gt;vendor&lt;/code&gt;: The company name.&lt;/li&gt; &lt;li&gt;&lt;code&gt;version&lt;/code&gt;: The version of the image.&lt;/li&gt; &lt;li&gt;&lt;code&gt;release&lt;/code&gt;: A number that's used to identify the specific build for this image.&lt;/li&gt; &lt;li&gt;&lt;code&gt;summary&lt;/code&gt;: A short overview of the application or component in this image.&lt;/li&gt; &lt;li&gt;&lt;code&gt;description&lt;/code&gt;: A longer description of the application or component in this image.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Set these labels in the Dockerfile using the &lt;code&gt;LABEL&lt;/code&gt; command. If you build your image on a UBI, Red Hat already sets these labels with Red Hat values for the UBI, but you should override those with values for your image.&lt;/p&gt; &lt;h2&gt;Best practice #7: Embed license information inside your image&lt;/h2&gt; &lt;p&gt;No industry standard exists for bundling the licensing information with software. However, you can easily store the text files for licenses in an image, so it's a good idea to do so. This makes the image self-documenting, so users can immediately know about the software licenses associated with their image.&lt;/p&gt; &lt;p&gt;GitHub can display the license for a repository if you follow these conventions:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://docs.github.com/en/github/creating-cloning-and-archiving-repositories/licensing-a-repository"&gt;Licensing a repository&lt;/a&gt; gives information about license types and encourages the owner of any open source, public repository to specify a license.&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.github.com/en/communities/setting-up-your-project-for-healthy-contributions/adding-a-license-to-a-repository"&gt;Adding a license to a repository&lt;/a&gt; explains that GitHub can detect and display the license for a repository if the license file is stored in the repository's home directory and named &lt;code&gt;LICENSE&lt;/code&gt; or &lt;code&gt;LICENSE.md&lt;/code&gt; (with all caps).&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Adding license information to an image&lt;/h3&gt; &lt;p&gt;The source code directory that contains the Dockerfile should also include a licenses directory that contains these licensing files. Typically, it contains at least one file with a name like &lt;code&gt;LICENSE.txt&lt;/code&gt;. The directory looks like this:&lt;/p&gt; &lt;pre class="language-bash"&gt; $ ls -ld licenses Dockerfile -rw-r--r-- 1 bwoolf staff 774 May 5 15:07 Dockerfile drwxr-xr-x 3 bwoolf staff 96 May 5 15:09 licenses $ ls -l licenses total 8 -rw-r--r-- 1 bwoolf staff 17 May 5 15:10 LICENSE.txt&lt;/pre&gt; &lt;p&gt;The following code in the Dockerfile adds this licenses directory to the image:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;COPY licenses /licenses&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Red Hat Container Certification requirement&lt;/h3&gt; &lt;p&gt;Red Hat requires that the image store the license file(s) in the &lt;code&gt;/licenses&lt;/code&gt; directory. It's convenient to create a corresponding licenses directory in the repository's home directory that the Dockerfile will copy as-is into the image.&lt;/p&gt; &lt;p&gt;To accommodate GitHub and Red Hat's different approaches, you can store two copies of your license file, one in &lt;code&gt;LICENSE&lt;/code&gt; for GitHub and another in &lt;code&gt;licenses/LICENSE.txt&lt;/code&gt; for Red Hat.&lt;/p&gt; &lt;h2&gt;Best practice #8: Maintain the original base image layers&lt;/h2&gt; &lt;p&gt;When building an application image, do not modify, replace, or combine the packages or layers in the base image. However, there is one exception: The build process can and should update the security packages in the Linux libraries with the latest updates.&lt;/p&gt; &lt;p&gt;A container image's metadata should clearly show that the image includes the layers of the base image, has not altered them, and has only added to them.&lt;/p&gt; &lt;h3&gt;Maintaining the original image layers in your Dockerfile&lt;/h3&gt; &lt;p&gt;A Dockerfile normally builds from a base image and adds new layers to it. An application should run on top of its operating system, but not replace any of it.&lt;/p&gt; &lt;h3&gt;Red Hat Container Certification requirement&lt;/h3&gt; &lt;p&gt;Red Hat Container Certification prohibits modifications to the layers in the Red Hat base image. When the base layer uses a Red Hat UBI, Red Hat does support the use and extension of the UBI layer.&lt;/p&gt; &lt;p&gt;See the &lt;a href="https://access.redhat.com/articles/2726611"&gt;Red Hat Container Support Policy&lt;/a&gt; for more information.&lt;/p&gt; &lt;h2&gt;Best practice #9: Limit the number of layers in your images&lt;/h2&gt; &lt;p&gt;Layers in an image are good, but having too many adds complexity and hurts efficiency. Limit the images you build to about 5-20 layers (including the base image's layers). Up to 30 layers are acceptable, but 40 or more layers become too many to manage easily.&lt;/p&gt; &lt;h3&gt;Limiting the number of layers&lt;/h3&gt; &lt;p&gt;The number of layers in an image depends on how the image is built. To list the layers in an image, use the &lt;a href="https://docs.docker.com/engine/reference/commandline/cli/"&gt;Docker command-line interface&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;docker history &lt;container_image_name&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Alternatively, use &lt;a href="https://podman.io/"&gt;Podman&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt; podman history &lt;container_image_name&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For example, the &lt;code&gt;ubi8/openjdk-11:1.3-15&lt;/code&gt; image has three layers:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ docker pull registry.access.redhat.com/ubi8/openjdk-11:1.3-15 $ docker history registry.access.redhat.com/ubi8/openjdk-11:1.3-15 IMAGE CREATED CREATED BY SIZE COMMENT a9937ea40626 7 days ago 509MB &lt;missing&gt; 13 days ago 4.7kB &lt;missing&gt; 13 days ago 103MB Imported from -&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Red Hat Container Certification requirement&lt;/h3&gt; &lt;p&gt;Red Hat Container Certification requires that the image contain fewer than 40 layers. Red Hat's UBIs have very few layers, enabling your build process to add many more layers without exceeding 40.&lt;/p&gt; &lt;h2&gt;Where to learn more&lt;/h2&gt; &lt;p&gt;By following the nine best practices in this article, you can build an image that is high-quality and efficient, and runs well in both Kubernetes and OpenShift. Here are a few additional resources:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Explore &lt;a href="https://connect.redhat.com/partner-with-us/red-hat-container-certification"&gt;Red Hat Container Certification&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Get a quick overview of &lt;a href="https://developers.redhat.com/products/rhel/ubi"&gt;Red Hat Universal Base Images&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Download the free e-book: &lt;em&gt;&lt;a href="https://developers.redhat.com/e-books/red-hat-universal-base-images-ubi"&gt;Red Hat Universal Base Images (UBI)&lt;/a&gt;&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/11/11/best-practices-building-images-pass-red-hat-container-certification" title="Best practices for building images that pass Red Hat Container Certification"&gt;Best practices for building images that pass Red Hat Container Certification&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>clobner</dc:creator><dc:date>2021-11-11T10:00:00Z</dc:date></entry><entry><title>Simplify Kafka authentication with Node.js</title><link rel="alternate" href="https://developers.redhat.com/articles/2021/11/11/simplify-kafka-authentication-nodejs" /><author><name>Michael Dawson</name></author><id>8c2ec74a-138b-4e3d-b660-b82a416fa082</id><updated>2021-11-11T07:00:00Z</updated><published>2021-11-11T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/topics/kafka-kubernetes"&gt;Apache Kafka&lt;/a&gt; is a publish-subscribe messaging system that is commonly used to build loosely coupled applications. These types of applications are often referred to as &lt;em&gt;reactive applications&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;Our team maintains a &lt;a href="https://github.com/nodeshift-starters/reactive-example"&gt;reactive example&lt;/a&gt; that shows the use of &lt;a href="https://developers.redhat.com/topics/kafka-kubernetes"&gt;Kafka&lt;/a&gt; in a simple application. If you've looked at these types of applications, you know that although the components are decoupled, they need access to a shared Kafka instance. Access to this shared instance must be protected. This means that each component needs a set of security credentials that it can use to connect to the Kafka instance.&lt;/p&gt; &lt;p&gt;As a &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt; developer, how can you safely share and use those credentials without a lot of work? Read on to find out.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; You can learn more about using Node.js in reactive applications in the article &lt;a href="https://developers.redhat.com/articles/2021/08/31/building-reactive-systems-nodejs"&gt;Building reactive systems with Node.js&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Connect to a Kafka instance&lt;/h2&gt; &lt;p&gt;The information you need to connect to a Kafka instance generally includes the following:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;One or more URLs for the Kafka cluster.&lt;/li&gt; &lt;li aria-level="1"&gt;Information about the connection/authentication mechanism.&lt;/li&gt; &lt;li aria-level="1"&gt;A user ID.&lt;/li&gt; &lt;li aria-level="1"&gt;A user secret.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;You definitely need to be careful about who has access to the last two, and ideally, you don’t want the first one to be public. The result is that you need to provide something like the following to connect to the Kafka instance:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;KAFKA_BOOTSTRAP_SERVER=michael--wmc-utegzxlxds-fhyltiy-j-j.bf2.kafka.cloud.com:443 KAFKA_CLIENT_ID=srvc-acct-123456789-1234-1234-1234-24bda5a97b89 KAFKA_CLIENT_SECRET=abcdef12-1234-1234-1234-da90b53e893e KAFKA_SASL_MECHANISM=plain&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Additionally, you should not expose the connection information beyond the application itself.&lt;/p&gt; &lt;p&gt;The other thing to note is that there are a number of different Kafka clients for Node.js, and the way you pass this information for each client is different. If you need help choosing which client to use, check out the &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture/blob/main/docs/functional-components/message-queuing.md#kafka"&gt;Kafka&lt;/a&gt; section in the &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture"&gt;Node.js reference architecture&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;As a simple example, to pass the information when using the &lt;a href="https://www.npmjs.com/package/kafkajs"&gt;KafkaJS&lt;/a&gt; client, you could use code like this:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;  kafkaConnectionBindings = {     brokers: [process.env.KAFKA_BOOTSTRAP_SERVER ]   };   if (process.env.KAFKA_SASL_MECHANISM === 'plain') {     kafkaConnectionBindings.sasl = {       mechanism: process.env.KAFKA_SASL_MECHANISM,       username: process.env.KAFKA_CLIENT_ID,       password: process.env.KAFKA_CLIENT_SECRET     };     kafkaConnectionBindings.ssl = true;   }   const kfk = new Kafka(kafkaConnectionBindings);&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Using environment variables is the easiest way to configure a connection, but it is not necessarily secure. If you set the environment variables from the command line, then anybody with access to the environment will be able to access them. Tools and frameworks will also often make it easy to access environment variables for debugging purposes. For example, in &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;, you can view the environment variables from the console, as shown in Figure 1.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/dawson_01_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/dawson_01_0.png?itok=9OwkiJ7g" width="852" height="513" alt="Viewing the environment variables from the console in Red Hat OpenShift." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1. Environment variables listed in the OpenShift console.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;In production environments, make sure that even if you trust those with access to the environments, only those who have a "need to know" have access to information like credentials.&lt;/p&gt; &lt;h2&gt;Passing the credentials securely&lt;/h2&gt; &lt;p&gt;Now that you understand what information you need to pass, how do you safely get the credentials to the running application?&lt;/p&gt; &lt;p&gt;Instead of setting the credentials in the environment directly, a safer way is to use a package like &lt;a href="https://www.npmjs.com/package/dotenv"&gt;dotenv&lt;/a&gt; to get the credentials from a file and provide them to the Node.js application environment. The benefit of using dotenv is that the credentials will not show up in the environment outside of the Node.js process.&lt;/p&gt; &lt;p&gt;While this approach is better, the credentials still might be exposed if you dump the Node.js environment for debugging through a &lt;a href="https://developer.ibm.com/articles/easily-identify-problems-in-your-nodejs-apps-with-diagnostic-report/"&gt;Node.js diagnostic report&lt;/a&gt;. You are also left with the question of how to get the dotenv file securely to the application. If you are deploying to &lt;a href="https://developers.redhat.com/topics/kubernetes/"&gt;Kubernetes&lt;/a&gt;, you can map a file into deployed &lt;a href="https://developers.redhat.com/topics/containers/"&gt;containers&lt;/a&gt;, but that will take some planning and coordination for developments.&lt;/p&gt; &lt;p&gt;By this point, you are probably thinking that this seems like a lot of work and are wondering whether you'd need to resolve this for each type of service and set of credentials that are needed by an application. The good news is that for Kubernetes environments, this problem has already been solved.&lt;/p&gt; &lt;h2&gt;Service binding in Kubernetes&lt;/h2&gt; &lt;p&gt;Service binding is a standard approach to map a set of files into containers in order to provide credentials in a safe and scalable way. You can read more about the Service Binding specification for Kubernetes on &lt;a href="https://github.com/k8s-service-bindings/spec"&gt;GitHub&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The specification does not define what files are mapped in for a given service type. But in OpenShift, for example, binding to a &lt;a href="https://developers.redhat.com/products/red-hat-openshift-streams-for-apache-kafka/getting-started"&gt;Red Hat OpenShift Streams for Apache Kafka&lt;/a&gt; instance results in the following files being mapped into the application container:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;​​​​​​​$SERVICE_BINDING_ROOT/&lt;kafka-instance-name&gt; ├── bootstrapServers ├── password ├── provider ├── saslMechanism ├── securityProtocol ├── type └── user&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In this case, &lt;code&gt;SERVICE_BINDING_ROOT&lt;/code&gt; is passed to the application through the environment.&lt;/p&gt; &lt;h2&gt;Consume service bindings easily with kube-service-bindings&lt;/h2&gt; &lt;p&gt;Now that you have the credentials available to the application running in the container, the remaining work is to read the credentials from those files and then provide them to the Kafka client used within your Node.js application. Hmm—that still sounds like a lot of work, and it's also tied to the client you are using.&lt;/p&gt; &lt;p&gt;More good news! We’ve put together the npm package &lt;a href="https://www.npmjs.com/package/kube-service-bindings"&gt;&lt;code&gt;kube-service-bindings&lt;/code&gt;&lt;/a&gt;, which makes it easy for Node.js applications to consume these secrets without requiring developers to be familiar with service bindings.&lt;/p&gt; &lt;p&gt;The package provides the &lt;code&gt;getBinding()&lt;/code&gt; method, which does roughly the following:&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Looks for the &lt;code&gt;$SERVICE_BINDING_ROOT&lt;/code&gt; variable in order to determine if bindings are available.&lt;/li&gt; &lt;li aria-level="1"&gt;Reads the info from the files.&lt;/li&gt; &lt;li aria-level="1"&gt;Maps the names of the files to the option names needed by the Node.js clients that will connect to the service.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;Figure 2 is a nice picture of that flow.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/dawson_02.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/dawson_02.png?itok=a9UW-4wh" width="1191" height="707" alt="An illustrated flow of the getBinding() method." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2. Consuming service bindings with the kube-service-bindings package.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;All the developer needs to do is call the &lt;code&gt;getBinding()&lt;/code&gt; method, tell it which client it is using, and then pass the returned object to their Kafka client. Minus error checking, the code is as simple as this:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;const serviceBindings = require('kube-service-bindings'); ​​​​​​​try {   kafkaConnectionBindings =      serviceBindings.getBinding('KAFKA', 'kafkajs'); } catch (err) { // proper error handling here }; const kfk = new Kafka(kafkaConnectionBindings);&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The first parameter is &lt;code&gt;KAFKA&lt;/code&gt; because we are talking about connecting to a Kafka service. But in the future, &lt;code&gt;kube-service-bindings&lt;/code&gt; should help bind to other types of services as well.&lt;/p&gt; &lt;p&gt;The &lt;a href="https://github.com/nodeshift-starters/reactive-example"&gt;reactive-example&lt;/a&gt; is coded so that it can get the credentials from the environment, a dotenv file, or automatically when the credentials are available through service bindings. There are two branches in the repository: &lt;a href="https://github.com/nodeshift-starters/reactive-example/tree/kafkajs"&gt;&lt;code&gt;kafkajs&lt;/code&gt;&lt;/a&gt; and &lt;a href="https://github.com/nodeshift-starters/reactive-example/tree/node-rdkafka"&gt;&lt;code&gt;node-rdkafka&lt;/code&gt;&lt;/a&gt;. These branches let you see what the code looks like for your preferred client and how &lt;code&gt;kube-service-bindings&lt;/code&gt; gives you the credentials in the format needed by that client.&lt;/p&gt; &lt;h2&gt;Set up service bindings in OpenShift&lt;/h2&gt; &lt;p&gt;So far, you've seen that with &lt;code&gt;kube-service-bindings&lt;/code&gt;, it's easy for Node.js developers to use credentials available through service bindings.&lt;/p&gt; &lt;p&gt;The second part is to set up the service bindings themselves. The article &lt;a href="https://developers.redhat.com/articles/2021/07/27/connect-nodejs-applications-red-hat-openshift-streams-apache-kafka-service#"&gt;Connect Node.js applications to Red Hat OpenShift Streams for Apache Kafka with Service Binding&lt;/a&gt; takes you through the steps of setting up service bindings to connect a Node.js application with an instance of &lt;a href="https://developers.redhat.com/products/red-hat-openshift-streams-for-apache-kafka/getting-started"&gt;Red Hat OpenShift Streams for Apache Kafka&lt;/a&gt;. As you might expect in a Kubernetes environment, you first install some Operators. Then, you apply some YAML to tell one of these Operators to bind the OpenShift Streams for Apache Kafka instance to your application.&lt;/p&gt; &lt;p&gt;What's even better is that with the release of &lt;a href="https://docs.openshift.com/container-platform/4.8/release_notes/ocp-4-8-release-notes.html"&gt;OpenShift 4.8&lt;/a&gt;, you can use the OpenShift UI to do this binding! This gets us to the point where the administrator/operators of the cluster can easily set up the Kafka instance for an organization. Developers can then connect their applications without needing to know the credentials. The UI can be used for ease of use during initial development, and then YAML can be used for more automated/production deployments.&lt;/p&gt; &lt;p&gt;To see for yourself just how easy this is, follow these steps to connect an application to a configured Kafka instance:&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Hover the mouse pointer over the icon for the application (consumer-backend, which is one of the components in the &lt;a href="https://github.com/nodeshift-starters/reactive-example"&gt;reactive-example&lt;/a&gt;), as shown in Figure 3. &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/dawson_03_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/dawson_03_0.png?itok=bDe3bO5p" width="665" height="163" alt="Screenshot of the icon for the application (consumer-backend)." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 3. The consumer-backend application icon in the OpenShift UI.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Left-click and drag the head of the arrow until it's directly over the icon for the &lt;code&gt;KafkaConnection&lt;/code&gt; object (Figure 4). &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/dawson_04.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/dawson_04.png?itok=mCEeQuLv" width="671" height="198" alt="Left-click and drag the head of the arrow until it's directly over the icon for the KafkaConnection object to create the service binding." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 4. The KafkaConnection object in the OpenShift UI.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Release the left mouse button to create the service binding.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;That’s it! If your code uses &lt;code&gt;kube-service-bindings&lt;/code&gt;, it will automatically find the credentials through the binding and connect to the server for your Kafka instance.&lt;/p&gt; &lt;h2&gt;Node.js and Apache Kafka: Further resources&lt;/h2&gt; &lt;p&gt;In this article, we introduced the credentials needed to connect to a Kafka server and showed you how they can be safely provided to your Node.js applications. If you want to dive deeper, try the following:&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Install and experiment with the &lt;a href="https://github.com/nodeshift-starters/reactive-example"&gt;reactive-example&lt;/a&gt; to explore the code and &lt;a href="https://www.npmjs.com/package/kube-service-bindings"&gt;&lt;code&gt;kube-service-bindings&lt;/code&gt;&lt;/a&gt;. (If you are really adventurous, you could create your own files and set &lt;code&gt;SERVICE_BINDING_ROOT&lt;/code&gt; to point to them.)&lt;/li&gt; &lt;li aria-level="1"&gt;Work through setting up service bindings for a Kafka instance in &lt;a href="https://developers.redhat.com/articles/2021/07/27/connect-nodejs-applications-red-hat-openshift-streams-apache-kafka-service#"&gt;Connect Node.js applications to Red Hat OpenShift Streams for Apache Kafka with Service Binding&lt;/a&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;Work through the quick start of &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_openshift_streams_for_apache_kafka/1/guide/5b6badee-eaf3-4a10-b5ec-57dc6b94ec0f"&gt;manually connecting Node.js to Kafka&lt;/a&gt; published on the Red Hat customer portal.&lt;/li&gt; &lt;li aria-level="1"&gt;Work through the tutorial for &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_openshift_streams_for_apache_kafka/1/guide/5d3559cc-1989-41b7-a2bc-e461d2337a6f#_d6073c03-5da2-425e-ac5d-ba13ac73c39d"&gt;automatically binding Node.js to Kafka&lt;/a&gt; published on the Red Hat customer portal.&lt;/li&gt; &lt;li aria-level="1"&gt;If you have installed the RHOAS operator, work through the quick start for &lt;a href="https://github.com/redhat-developer/app-services-operator/blob/main/olm/quickstarts/rhosak-openshift-nodejs-bind-quickstart.yaml"&gt;automatically binding Node.js&lt;/a&gt;.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;To stay up to date with what Red Hat is up to on the Node.js front, check out our &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js landing page&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/11/11/simplify-kafka-authentication-nodejs" title="Simplify Kafka authentication with Node.js"&gt;Simplify Kafka authentication with Node.js&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Michael Dawson</dc:creator><dc:date>2021-11-11T07:00:00Z</dc:date></entry></feed>
